/* Copyright 2023 Michael Sippel
 *
 * This Source Code Form is subject to the terms of the Mozilla Public
 * License, v. 2.0. If a copy of the MPL was not distributed with this
 * file, You can obtain one at http://mozilla.org/MPL/2.0/.
 */

#pragma once

#include <atomic>
#include <cstddef>
#include <cstdint>
#include <fmt/format.h>
#include <memory>
#include <optional>

#include <hwloc.h>
#include <spdlog/spdlog.h>

#include <redGrapes/memory/block.hpp>

namespace redGrapes
{
namespace memory
{

/* maintains a lockfree singly-linked list
 * with the following allowed operations:
 *   - append new chunks at head
 *   - erase any chunk which is not current head
 *   - reversed iteration (starting at head)
 *
 * each chunk is managed through a `std::shared_ptr` which points to a
 * contiguous block containing list-metadata, the chunk-control-object
 * (`ChunkData`) and freely usable data.
 *
 * @tparam Item element type
 * @tparam Allocator must satisfy `Allocator` concept
 */
template <
    typename Item,
    typename Allocator
>
struct AtomicList
{
//private:
    struct ItemControlBlock
    {
        bool volatile deleted;
        std::shared_ptr< ItemControlBlock > prev;
        uintptr_t item_data_ptr;

        ItemControlBlock( memory::Block blk )
            : deleted( false )
            , item_data_ptr( blk.ptr )
        {
            /* put Item at front and initialize it
             * with the remaining memory region
             */
            blk.ptr += sizeof(Item);
            blk.len -= sizeof(Item);
            new ( get() ) Item ( blk );
        }

        ~ItemControlBlock()
        {
            get()->~Item();
        }

        /* flag this chunk as deleted and call ChunkData destructor
         */
        void erase()
        {
            deleted = true;
        }

        /* adjusts `prev` so that it points to a non-deleted chunk again
         * this should free the shared_ptr of the original prev
         * in case no iterators point to it
         */
        void skip_deleted_prev()
        {
            std::shared_ptr<ItemControlBlock> p = std::atomic_load( &prev );
            while( p && p->deleted )
                p = std::atomic_load( &p->prev );

            std::atomic_store( &prev, p );
        }

        Item * get() const
        {
            return (Item*)item_data_ptr;
        }
    };

    Allocator alloc;
    std::shared_ptr< ItemControlBlock > head;
    size_t const chunk_capacity;

    /* keeps a single, predefined pointer
     * and frees it on deallocate.
     * used to spoof the allocated size to be bigger than requested.
     */
    template <typename T>
    struct StaticAlloc
    {
        typedef T value_type;

        Allocator alloc;
        T * ptr;

        StaticAlloc( Allocator alloc, size_t n_bytes )
            : alloc(alloc)
            , ptr( (T*)alloc.allocate( n_bytes ) )
        {}

        template<typename U>
        constexpr StaticAlloc( StaticAlloc<U> const & other ) noexcept
            : alloc(other.alloc)
            , ptr((T*)other.ptr)
        {}

        T * allocate( size_t n ) noexcept
        {
            return ptr;
        }

        void deallocate( T * p, std::size_t n ) noexcept
        {
            alloc.deallocate( Block{ .ptr=(uintptr_t)p, .len=sizeof(T)*n} );
        }
    };

public:
    AtomicList( Allocator && alloc, size_t chunk_capacity )
        : alloc( alloc )
        , head( nullptr )
        , chunk_capacity( chunk_capacity )
    {
    }

    static constexpr size_t get_controlblock_size()
    {
        /* TODO: use sizeof( ...shared_ptr_inplace_something... )
         */
        size_t const shared_ptr_size = 512;
        return sizeof(ItemControlBlock) + shared_ptr_size;
    }

    constexpr size_t get_chunk_capacity()
    {
        return chunk_capacity;
    }

    constexpr size_t get_chunk_allocsize()
    {
        return chunk_capacity + get_controlblock_size();
    }

    /* allocate a new item and add it to the list
     *
     * @{
     */
    auto allocate_item()
    {
        TRACE_EVENT("Allocator", "AtomicList::allocate_item()");

        /* NOTE: we are relying on std::allocate_shared
         * to do one *single* allocation which contains:
         * - shared_ptr control block
         * - chunk control block
         * - chunk data
         * whereby chunk data is not included by sizeof(ItemControlBlock),
         * but reserved by StaticAlloc.
         * This works because shared_ptr control block lies at lower address.
         */
        StaticAlloc<void> chunk_alloc( this->alloc, get_chunk_allocsize() );

        // this block will contain the Item-data of ItemControlBlock
        memory::Block blk{
            .ptr = (uintptr_t)chunk_alloc.ptr + get_controlblock_size(),
            .len = chunk_capacity - get_controlblock_size()
        };

        return append_item( std::allocate_shared< ItemControlBlock >( chunk_alloc, blk ) );
    }

    /** allocate the first item if the list is empty
     *
     * If more than one thread tries to add the first item only one thread will successfully add an item.
     */
    bool try_allocate_first_item()
    {
        TRACE_EVENT("Allocator", "AtomicList::allocate_first_item()");
        StaticAlloc<void> chunk_alloc( this->alloc, get_chunk_allocsize() );

        // this block will contain the Item-data of ItemControlBlock
        memory::Block blk{
            .ptr = (uintptr_t)chunk_alloc.ptr + get_controlblock_size(),
            .len = chunk_capacity - get_controlblock_size()
        };

        auto sharedChunk = std::allocate_shared< ItemControlBlock >( chunk_alloc, blk );
        return try_append_first_item(  std::move(sharedChunk) );
    }
    /** @} */

    template < bool is_const = false >
    struct BackwardIterator
    {
        std::shared_ptr< ItemControlBlock > c;

        void erase()
        {
            c->erase();
        }

        bool operator!=(BackwardIterator const & other) const
        {
            return c != other.c;
        }

        operator bool() const
        {
            return (bool)c;
        }

        typename std::conditional<
            is_const,
            Item const *,
            Item *
        >::type
        operator->() const
        {
            return c->get();
        }
        
        typename std::conditional<
            is_const,
            Item const &,
            Item &
        >::type
        operator*() const
        {
            return *c->get();
        }

        void optimize()
        {
            if(c)
                c->skip_deleted_prev();
        }

        BackwardIterator& operator++()
        {
            if( c )
            {
                c->skip_deleted_prev();
                c = c->prev;
            }

            return *this;
        }
    };

    using ConstBackwardIterator = BackwardIterator< true >;
    using MutBackwardIterator = BackwardIterator< false >;

    /* get iterator starting at current head, iterating backwards from
     * most recently added to least recently added
     */
    MutBackwardIterator rbegin() const
    {
        return MutBackwardIterator{ std::atomic_load(&head) };
    }

    MutBackwardIterator rend() const
    {
        return MutBackwardIterator{ std::shared_ptr<ItemControlBlock>() };
    }

    ConstBackwardIterator crbegin() const
    {
        return ConstBackwardIterator{ std::atomic_load(&head) };
    }

    ConstBackwardIterator crend() const
    {
        return ConstBackwardIterator{ std::shared_ptr<ItemControlBlock>() };
    }

    /* Flags chunk at `pos` as erased. Actual removal is delayed until
     * iterator stumbles over it.
     *
     * Since we only append to the end and `chunk` is not `head`,
     * there wont occur any inserts after this chunk.
     */
    void erase( MutBackwardIterator pos )
    {
        pos.erase();
    }

    /* atomically appends a floating chunk to this list
     * and returns the previous head to which the new_head
     * is now linked.
     */
    auto append_item( std::shared_ptr< ItemControlBlock > new_head )
    {
        TRACE_EVENT("Allocator", "AtomicList::append_item()");
        std::shared_ptr< ItemControlBlock > old_head;

        bool append_successful = false;
        while( ! append_successful )
        {
            old_head = std::atomic_load( &head );
            std::atomic_store( &new_head->prev, old_head );
            append_successful = std::atomic_compare_exchange_strong<ItemControlBlock>( &head, &old_head, new_head );
        }

        return MutBackwardIterator{ old_head };
    }

    // append the first head item if not already exists
    bool try_append_first_item( std::shared_ptr< ItemControlBlock > new_head )
    {
        TRACE_EVENT("Allocator", "AtomicList::append_first_item()");

        std::shared_ptr< ItemControlBlock > expected( nullptr );
        std::shared_ptr< ItemControlBlock > const & desired = new_head;
        return std::atomic_compare_exchange_strong<ItemControlBlock>( &head, &expected, desired );
    }
};

} // namespace memory

} // namespace redGrapes

